NOTE: README must be updated to reflect new variable naming scheme!

# Project Summary

Enrichment analysis algorithms identify known, annotated gene sets which share many genes in common with an input gene set. The input gene set might be a list of differentially-expressed genes from a biological experiment (e.g. treating both a cancerous cell line and a healthy cell line with a drug, and identifying genes with the greatest difference in expression levels using RNA-seq), while the annotations of the known gene sets might be transcription factor binding sites, drug targets, or biological processes. By associating the annotations of the most similar known gene sets to the input gene set, a scientist can use enrichment analysis to learn about the biological interconnectedness of the input gene set. 

The purpose of this project is to evaluate new algorithms for enrichment analysis. It only considers algorithms which begin with an input gene set, and not with the original gene expression data. It evaluates these algorithms by benchmarking their results using annotated transcription factor-to-gene or drug-to-gene libraries as inputs. 

The benchmarking process is as follows: Two libraries, A and B, are chosen. Terms which correspond to transcription factors (or drugs) which are present in both libraries are identified. Then, one at a time, the gene sets of those common terms from library A are used as input gene sets to perform enrichment into all of library B. For each common term in library A, the algorithms return a ranking of all terms in library B (the known gene sets), from most enriched to least enriched. Since these libraries should agree with one another, we expect that the library B term(s) corresponding to the same transcription factor (or drug) as the input term from library A will appear toward the top of the rankings. Therefore, a good algorithm will consistently rank these matching terms highly, while a bad algorithm will not. "Bridge plots" are created to qualitatively evaulate this.

[Here is a link to a video explanation of this project from August 2017.](https://www.youtube.com/watch?v=IyDvGPtHP7U)

## Algorithms

These enrichment analysis algorithms have been implemented:

* A control, which returns a random ranking of the known gene sets
* Fisher's exact test
* Binomial proportions test
* Z score and Combined score from the Enrichr API
* Gini Impurity and Entropy
	* A pairwise variant of the above methods, which splits on two terms at a time
* Machine learning methods, which build a classifier that predicts whether genes in library B also belong to the input gene set, and then uses feature importances to rank the library B gene sets:
	* RandomForest, ExtraTrees, RandomTreesEmbedding, GradientBoosting and AdaBoost from sklearn.ensemble
	* LinearSVC from sklearn.svm
	* XGBoost from the DMLC
	* Recursive variants of the above machine learning methods
	* *Deprecated: Above machine learning methods, but using a classifier that predicts whether or not a gene set is enriched based on Fisher contingency table values and other scores (e.g. Fisher p value, Gini impurity)*
* Mixtures of the above methods, i.e. score averaging and gene set elimination.
* *Deprecated: Fisher's exact test adjusted with gene-gene correlation data from ARCHS4* 

## Visualizations

These visualizations are available: 

* All bridge plots for a single library pair, comparing the rankings of different methods.
* A combined plot showing all bridge plots for all library pairs.
* An array of bridge plots, with each library pair in its own subplot.
* An array of hexbin plots (a kind of bivariate histogram), with each library pair in its own subplot.
	* These are useful for comparing how two different algorithms rank the same terms.

The bridge plots can be zoomed-in to view only the top ranks, and filtered to view only certain methods.

<div style="display:inline">
<img src="README images/sklearn.png"/>
<img src="README images/drugs.png"/>
</div>
<div style="display:inline">
<img src="README images/hexbin.png"/>
<img src="README images/grid.png"/>
</div>

## Libraries

The following libraries are used for benchmarking:

* Transcription factor libraries:
	* [ChEA](https://www.ncbi.nlm.nih.gov/pubmed/20709693): infers transcription factor binding sites using ChIP-X experiment data manually collected from scientific publications.
	* [ENCODE](https://www.encodeproject.org/): infers transcription factor binding sites using ChIP-seq experiments performed by the ENCODE project.
	* [CREEDS](http://amp.pharm.mssm.edu/CREEDS/): infers single-gene perturbation signatures using microarray data extracted and processed from the Gene Expression Omnibus by crowdsource.
* Drug libraries:
	* [DrugBank](https://www.drugbank.ca/): encyclopedia-like database of drugs and their targets based on scientific publications.
	* [CREEDS](http://amp.pharm.mssm.edu/CREEDS/): infers drug-gene associations using drug perturbation signatures extracted and processed from the Gene Expression Omnibus by crowdsource.

# How to Use:

1. Run `setup.py`.
2. `get_scores.py` has a function called `get_methods()`. In here, fill a `pandas.DataFrame` with functions and parameters from `enrichment_methods.py` which you want to use. Then, run `get_scores.py`.
3. Run `evaluate_scores.py` to save ranking scores to results files, and to plot the rankings.
* `fix.py` can be used to make adjustments such as renaming methods, reversing scores, and resetting the results files.
* `log.py` is a log of experiments I've personally done, for my own reference. 

Scripts I have used previously are in the `old scripts` folder. To use these, move them to the main directory. They are deprecated and may no longer work:
* `get_pairwise_fisher_pvals.py` and `analyze_pairwise_fisher_pvals.py` were used to perform and explore the results of pairwise enrichment analysis (evaluating the degree of enrichment of a pair of terms at a time, instead of a single term) using the Fisher's exact test.
* `create_abridged_libs.py` makes a new gmt which has only the first n terms of an old gmt. This was used to quickly test out enrichment methods that take a long time to run, such as the pairwise methods.
* `edgelist_to_gmt.py` was used to create gmts from drug library edgelists.
* `new_scores_from_old.py` can be used to create new scores from old score files.
* `get_classifiers.py` is a helper function needed if a `ML_Fisher_features` method is used. 
* `get_fisher_adjusted_vars.py` was used to get and view the correlation data for the adjusted Fisher's test.

# More details:

(Note: If drug libraries are being used, simply replace "transcription factor" with "drug" in the following passages.)

### What happens behind the scenes, in pseudocode:

1. **`setup.py`**
* The gmt files are transformed into gene-vector format, and other necessary files are downloaded and processed.

2. **`get_scores.py`**
* For each `(label, feature)` pair of gmt libraries:
	* Get a list of experiments in the label library whose corresponding transcription factor is also found in at least one feature library experiment. 
	* (From here on, this list will be referred to as the label library transcription factors)
	* For each enrichment method:
		* For each label library transcription factor:
			* Perform enrichment by inputting this label library transcription factor's gene set into the whole feature library.
			* The output is a list of enrichment scores for each experiment in the feature library.
			* Save this output as a column in a dataframe. 
		* The dataframe is stored as a `.csv` file. (Columns: label library transcription factors. Indices: feature library experiments. Cell values: scores given by the enrichment method.) Example: `from_ChEA_2016_to_CREEDS_Fisher.csv`

3. **`evaluate_scores.py`**
* For each `(label, feature)` pair of gmt libraries:
	* The corresponding `.csv` files from `get_scores.py` are loaded.
	* For each corresponding `.csv file` (i.e., for each method):
		* For each column (i.e., for each label library transcription factor):
			* The ranks for experiments which correspond to the same transcription factor used to perform enrichment are aggregated.
		* The coordinates of the bridge plot are generated, and saved to the rankings file. Example: `rankings_from_ChEA_2016_to_CREEDS.csv`
* Methods are grouped as desired (for example, by library or enrichment algorithm), then plotted.


### Naming conventions:
* `f` : file
* `df` : pandas.DataFrame
* `lib` : gmt library
* `l` : the "label" gmt library
* `f` : the "feature" gmt library
* `experiment` : The first column values of the original gmt files. These could be ChIP-seq experiments, single-gene perturbations, etc. 
* `tf(s)` : Transcription factor(s). Note: for some gmt files, not all experiments correspond to a tf
* `funct` : function - specifically, an enrichment analysis function in `gsea_methods.py`
* `params` : parameters for the function
* `method` : a specific function & paramaters combination which is being evaluated


### What is meant by "label" and "feature" library?:
The `label library` is the library whose tf gene sets are being inputted for enrichment analysis.
The `feature library` is the library which is being used as the background for enrichment analysis.
In other words, we take each tf from the `label library`, and perform enrichment with it using all of the `feature library`.
(Think of ML notation: genes are the samples, and membership within each `feature library` gene set are the features. We want to build a classifier which can label whether or not the gene belongs to the `label library` tf gene set. From this classifier, we extract the feature importances to get the list of enriched `feature library` tfs.)


### Construction and interpretation of the bridge plots:

<div style="display:inline">
<img src="README images/auc.png" width="600"/>
</div>

* The ranks of the "matching" experiments are collected.
* At every x-value after zero, the y-value decreases at a constant rate.
* At every x-value, the y value also increases proportionally to the number of "matching" ranks which occured at that x-value.
* The decrease and increase constants are scaled such that they cancel out, and the curve ends at y=0.
* The y-axis is scaled such that the highest y-value possible (if all ranks occured at x=0) is 1.
* The x-axis is scaled such that all the ranks are between one and zero.
* The result is a curve whose null-distribution is similar, but not identical, to a Brownian bridge. 

You can interpret the bridge plot as a cdf turned 45 degrees counter-clockwise. 

A good ranking method will result in a bridge plot with a large area underneath it (AUC), and with a tall supremum occuring at a low x-value. (In the best possible outcome, where all ranks occur at x=0, the curve will be an isosceles right triangle with AUC=.5 and supremum of 1 at x=0.)

# Future Directions

* Document gmt file creation for drug libraries
* Automate and continue hyperparmater search
* More combos of methods
* Linear SVC, KNN for ML on Fisher info
* Validation - just use p vals for ML on Fisher info
* Pairwise feature importance selection
