# Project Summary

Enrichment analysis algorithms identify known, annotated gene sets which are most similar to an input gene set. The input gene set might be a list of differentially-expressed genes from a biological experiment (e.g. treating a cancerous cell line with a drug, and identifying genes with the greatest change in expression levels using RNA-seq), while the annotations of the known gene sets might correspond to transcription factor binding sites identified by ChIP-seq, drug targets determined by perturbation signatures, or biological processes mined from scientific publications. By associating the annotations of the most similar known gene sets to the input gene set, a scientist can use enrichment analysis to learn about the biology of the input gene set. For the cancerous cell line example, this means answering questions like "What transcription factors might this drug have activated?", "What drugs have similar effects?", or "What regulatory pathways are playing a role here?".

The primary purpose of this project is to evaluate new algorithms for enrichment analysis. It only considers algorithms whose input is a gene set, and not original gene expression data. It evaluates these algorithms by measuring their abilities to, when given an annotated gene set from one library, return the gene sets in a another library whose annotations match that of the input.

[Here is a link to a video explanation of this project from August 2017.](https://www.youtube.com/watch?v=IyDvGPtHP7U) Note that some naming conventions have since changed: for example, I've renamed the feature and label libraries as the input and search libraries, respectively. 

## Algorithms

These enrichment analysis algorithms have been implemented:

* A control, which returns a random ranking of the known gene sets
* Fisher's exact test
* Binomial proportions test
* Z score and Combined score from the Enrichr API
* Gini Impurity and Entropy
	* A pairwise variant of the Gini Impurity and Entropy, which splits on two terms at a time
* Machine learning methods, which build classifiers that predict whether genes in the search library also belong to the input gene set, and then use feature importances to rank the search library annotations:
	* RandomForest, ExtraTrees, RandomTreesEmbedding, GradientBoosting and AdaBoost from sklearn.ensemble
	* LinearSVC from sklearn.svm
	* XGBoost from the DMLC
	* Recursive variants of the above machine learning methods
	* *Removed: Above machine learning methods, but using a classifier that predicts whether or not a gene set is enriched based on Fisher contingency table values and other metrics (e.g. Fisher p value, Gini impurity)*
* Mixtures of the above algorithms, i.e. score averaging and iterative elimination.
* *Removed: Fisher's exact test adjusted with gene-gene correlation data from ARCHS4* 

## Libraries

This project supports both transcription factor libraries and drug libraries.  

* Transcription factor libraries:
	* [ChEA](https://www.ncbi.nlm.nih.gov/pubmed/20709693): infers transcription factor binding sites using ChIP-X experiment data manually collected from scientific publications.
	* [ENCODE](https://www.encodeproject.org/): infers transcription factor binding sites using ChIP-seq experiments performed by the ENCODE project.
	* [CREEDS](http://amp.pharm.mssm.edu/CREEDS/): infers single-gene perturbation signatures using microarray data extracted and processed from the Gene Expression Omnibus by crowdsource.
* Drug libraries:
	* [DrugMatrix](http://www.niehs.nih.gov/outage_maintenance/): toxicogenomic database whose web tool is currently down until further notice.
	* [DrugBank](https://www.drugbank.ca/): encyclopedia-like database of drugs and their targets based on scientific publications.
	* [TargetCentral](http://juniper.health.unm.edu/tcrd/): database from the "Illuminating the Druggable Genome" project, which is compiled from multiple sources and emphasizes GPCR, kinase, ion channel and nuclear receptor targets. 
	* [CREEDS](http://amp.pharm.mssm.edu/CREEDS/): infers drug-gene associations using drug perturbation signatures extracted and processed from the Gene Expression Omnibus by crowdsource.

*Note: drug library compatibility is currently under construction.*

## Description of benchmarking process

The benchmarking process is as follows: Two libraries whose gene sets have similar annotations are chosen. (Either both have transcription factor annotations, or both have drug annotations.) One is designated as the `input library`, and the other is designated as the `search library`. Annotations in the `input library` whose corresponding transcription factor or drug also corresponds to at least one annotation in the `search library` are identified. Then, one at a time, the gene sets of these common annotations from the `input library` are used to perform enrichment into all of the `search library`. For each input gene set, the enrichment algorithm will return a ranking of all annotations in the `search library`, from most enriched to least enriched. Since the input and search libraries should agree with one another, we expect that the `search library` annotations corresponding to the same transcription factor (or drug) as the input annotation will appear toward the top of the rankings. Therefore, a good algorithm will consistently rank these matching annotations highly, while a bad algorithm will not. "Bridge plots," explained below, are used to qualitatively evaulate this.

Because the libraries are derived differently (e.g. transcription factor-to-gene libraries might be derived from experimental ChIP-seq data, or from ChIP-seq data mined from publications, or from microarray data), a secondary purpose of this project is to measure the agreement between the libraries. In theory, all libraries of the same type should agree with each other to some degree, and libraries which used the same or similar techniques should have even more agreement among one another.

## Visualizing the results

These visualizations are available: 

* All bridge plots for a single library pair, comparing the rankings of different algorithms.
* A combined plot showing all bridge plots for all library pairs.
* An array of bridge plots, with each library pair in its own subplot.
* An array of hexbin plots (a kind of bivariate histogram), with each library pair in its own subplot.
	* These are useful for comparing how two different algorithms rank the same terms.

The bridge plots can be zoomed-in to view only the top ranks, and filtered to view only certain algorithms.

<div style="display:inline">
<img src="README images/drugs.png" title="Bridge plot"/>
<img src="README images/bridgeplot.png" title="Bridge plot, zoomed in"/>
</div>
<div style="display:inline">
<img src="README images/grid.png" title="Bridge plot array"/>
<img src="README images/hexbin.png" title="Hexbin array"/>
</div>

### Construction and interpretation of the bridge plots:

<div style="display:inline">
<img src="README images/auc.png" width="500"/>
</div>

Bridge plots are used to view the rank positions of the matching annotations from the search library, aggregated over all input gene sets from the input library. 

* The ranks of the "matching" experiments are collected.
* At every x-value after zero, the y-value decreases at a constant rate.
* At every x-value, the y value also increases proportionally to the number of "matching" ranks which occured at that x-value.
* The decrease and increase constants are scaled such that they cancel out, and the curve ends at y=0.
* The y-axis is scaled such that the highest y-value possible (if all ranks occured at x=0) is 1.
* The x-axis is scaled such that all the ranks are between one and zero.
* The result is a curve whose null-distribution is similar, but not identical, to a Brownian bridge. 

You can sort of interpret the bridge plot as a cdf of the ranks of the matches, turned 45 degrees counter-clockwise. 

A good ranking method will result in a bridge plot with a large area underneath it (AUC), and with a tall supremum occuring at a low x-value. (In the best possible outcome, where all ranks occur at x=0, the curve will be an isosceles right triangle with AUC=.5 and supremum of 1 at x=0.)

# How to Use:

1. Run `setup.py` to format all libraries as gene vector matrices.
2. `perform_enrichment.py` has a function called `get_algorithms()`. In here, fill a data.frame with functions and parameters from `enrichment_functions.py` which you want to use. Also, choose which libraries you want to use. Then, run `perform_enrichment.py` to obtain scores for each algorithm, for each library pair. 
3. Run `evaluate_results.py` to obtain the rankings of the matching annotations, and to visualize the results.
* `fix.py` can be used to make adjustments such as renaming algorithms, resetting the results files, and modifying scores.
* `log.py` is a log of experiments I've personally done, for my own reference. 

Scripts I have used previously are in the `old scripts` folder. They worked with an older version and probably do not work now:
* `get_pairwise_fisher_pvals.py` and `analyze_pairwise_fisher_pvals.py` were used to perform and explore the results of pairwise enrichment analysis (evaluating the degree of enrichment of a pair of terms at a time, instead of a single term) using the Fisher's exact test.
* `create_abridged_libs.py` made a new gvm which has only the first n terms of another file. This was used to quickly test out enrichment algorithms that take a long time to run, such as the pairwise algorithms.
* `edgelist_to_gmt.py` was used to create gmts from drug library edgelists.
* `new_scores_from_old.py` was used to create new scores from old score files.
* `get_classifiers.py` was a helper function needed if a `ML_Fisher_features` method is used. 
* `get_fisher_adjusted_vars.py` was used to get and view the correlation data for the adjusted Fisher's test.

# Code details:

## Description of pipeline

1. **`setup.py`**
* The gmt files are transformed into gene vector matrices (gvms), with column labels being annotations and row labels being genes. Each column is a boolean vector corresponding to a gene set.

2. **`get_scores.py`**
* For each (input, search) pair of libraries:
	* Get a list of annotations in the index library whose corresponding transcription factor is also found in at least one search library annotations. These will be referred to as the common index library annotations. 
	* For each enrichment algorithm:
		* For each common index library annotation:
			* Perform enrichment by using this common annotation's gene set as the input gene set for the enrichment analysis algorithm.
			* The output of the algorithm is an enrichment score for each annotation in the search library.
			* Save this output as a column in a dataframe. 
		* The final dataframe is stored as a `.csv` file. (Columns: common index library annotations. Indices: search library annotations. Cell values: scores given by the enrichment method. Title of file: "input_", input library, "_into_", search library, ".csv".)

3. **`evaluate_scores.py`**
* For each `(input, search)` pair of gmt libraries:
	* The results files created by `get_scores.py` are loaded.
	* For each result file (i.e., for each algorithm):
		* For each column (i.e., for each common input library annotation):
			* The ranks for algorithms which correspond to the same transcription factor or drug as the input gene set annotation are aggregated.
		* Based on these ranks, the coordinates of the bridge plot are generated, and saved to the rankings file (so that ranks and coordinates only need to be calculated once). Example: `rankings_from_ChEA_2016_to_CREEDS.csv`
* Results are plotted as desired.

## Naming conventions:

* `fname` : file name
* `df` : pandas.DataFrame
* `gmt` : a file in [gene matrix transposed](http://software.broadinstitute.org/cancer/software/gsea/wiki/index.php/Data_formats#GMT:_Gene_Matrix_Transposed_file_format_.28.2A.gmt.29) format
* `gvm` : a df in "gene vector matrix" format, where columns are gene set labels i.e. annotations, rows are genes, and cell values are boolean and denote set membership. In this format, each column is a vector describing a gene set.  
* `lib` : a library
* `ilib` : the input library
* `slib` : the search library
* `annot` : annotation
* `tf` : transcription factor
* `method` : a machine learning method, for example `RandomForestClassifier`
* `funct` : function - specifically, an enrichment analysis function in `enrichment_functions.py`
* `params` : parameters for a function
* `algorithm` : a specific function & paramaters combination which is being used to perform enrichment
* `match` or `hit` : annotations which correspond to the same transcription factor or drug 

# Future Directions

* Enable use of drug libraries by identifying drug synonyms to make annotations from different libraries compatible
* Merge code changes from DrugLib Enrichment Comparison project
* Continue algorithm development
